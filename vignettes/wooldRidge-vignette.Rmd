---
title: "wooldRidge-vignette"
author: "Justin M Shea"
date: "`r Sys.Date()`"
output: html_vignette
pdf_document: default
vignette: >
  %\VignetteIndexEntry{wooldRidge-vignette}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

An excellent approach to learning is to find an example from your textbook
and then recreate it. Below are examples from every chapter and the syntax 
provided here should get you through most of the book.

Load the `wooldRidge` package to access data in the manner specified in each example.
```{r, echo = TRUE, eval = TRUE, warning=FALSE}
library(wooldRidge)
```

## Chapter 2: The Simple Regression Model

**`Example 2.10:` A Log Wage Equation**

From the text:

> " Using the wage1 data as in Example 2.4, but using log(wage) as the dependent variable, we obtain the following relationship:"

$$\widehat{log(wage)} = \beta_0 + \beta_1educ$$

First, load the `wage1` data.
```{r}
data(wage1)
```

Next, estimate a linear relationship between the log of _wage_ and _education_.

```{r}
log_wage_model <- lm(lwage ~ educ, data = wage1)
```

Finally, print the coefficients and $R^2$.

```{r}
log_wage_model$coefficients
summary(log_wage_model)$r.squared
```

## Chapter 3: Multiple Regression Analysis: Estimation

**`Example 3.2:` Hourly Wage Equation**

From the text:

> " Using the 526 observations on workers in 'wage1', we include $educ$(years of education), $exper$(years of labor market experience), and $tenure$(years with the current employer) in an equation explain log($wage$)."

$$\widehat{log(wage)} = \beta_0 + \beta_1educ + \beta_3exper + \beta_4tenure$$

Estimate the model regressing _education_, _experience_, and _tenure_ against _log(wage)_.
```{r}
hourly_wage_model <- lm(lwage ~ educ + exper + tenure, data = wage1)
```

Again, print the estimated model coefficients:
```{r}
hourly_wage_model$coefficients
```


## Chapter 4: Multiple Regression Analysis: Inference

**`Example 4.7` Effect of Job Training on Firm Scrap Rates**

From the text:

> " The scrap rate for a manufacturing firm is the number of defective items - products that must be discarded - out of every 100 produced. Thus, for a given number of items produced, a decrease in the scrap rate reflects higher worker productivity."

> "We can use the scrap rate to measure the effect of worker training on productivity. Using the data in jtrain, but only for the year 1987 and for non-unionized firms, we obtain the following estimated equation:"

First, load the `jtrain` data set.
```{r, echo = TRUE, eval = TRUE, warning=FALSE}
data("jtrain")
```

Next, create a logical index identifying which observations occur in 1987 and are non-union.

```{r} 
index <- jtrain$year == 1987 & jtrain$union == 0
```

Next, subset the jtrain data by the new index. This returns a data.frame of `jtrain` data of non-union firms for the year 1987.

```{r}
jtrain_1987_nonunion <- jtrain[index,]
```

Now create the linear model regressing hrsemp(total hours training/total employees trained), the log of annual sales, and the log of the number of the employees, against the log of the scrape rate.

$$lscrap = \alpha + \beta_1 hrsemp + \beta_2 lsales + \beta_3 lemploy$$


```{r}
linear_model <- lm(lscrap ~ hrsemp + lsales + lemploy, data = jtrain_1987_nonunion)
```

Finally, print the complete summary statistic diagnostics of the model.
```{r}
summary(linear_model)
```

## Chapter 5: Multiple Regression Analysis: OLS Asymptotics

**`Example 5.3:` Economic Model of Crime**

From the text:

> "We illustrate the Lagrange Multiplier $(LM)$ statistics test by using a slight extension of the crime model from example 3.5."

$$narr86 = \beta_0 + \beta_1pcnv + \beta_2avgsen + \beta_3tottime + \beta_4ptime86 + \beta_5qemp86 + \mu$$

$narr86:$ number of times arrested, 1986.

$pcnv:$ proportion of prior arrests leading to convictions.

$avgsen:$ average sentence served, length in months.

$tottime:$ time in prison since reaching the age of 18, length in months.

$ptime86:$ months in prison during 1986

$qemp86:$ quarters employed, 1986


Load the `crime1` data set containing arrests during the year 1986 and other information on 2,725 men born in either 1960 or 1961 in California.
```{r}
data(crime1)
```

From the text:

> "We use the $LM$ statistic to test the null hypothesis that $avgsen$ and $tottime$ have no effect on $narr86$ once other factors have been controlled for. First, estimate the restricted model by regressing $narr86$ on $pcnv, ptime86,$ and $qemp86$; the variables $avgsen$ and $tottime$ are excluded from this regression."

```{r, tidy = TRUE}
restricted_model <- lm(narr86 ~ pcnv + ptime86 + qemp86, data = crime1)
```

We obtain the residuals $\tilde{\mu}$ from this regression, 2,725 of them. 

```{r}
restricted_model_u <- restricted_model$residuals
```

Next, we run the regression of:

$$\tilde{\mu} = \beta_1pcnv + \beta_2avgsen + \beta_3tottime + \beta_4ptime86 + \beta_5qemp86$$
From the text:

> "As always, the order in which we list the independent variables is irrelevant.This second regression produces $R^2_{\mu}$, which turns out to be about 0.0015."

```{r, tidy = TRUE}
LM_u_model <- lm(restricted_model_u ~ pcnv + ptime86 + qemp86 + avgsen + tottime, data = crime1)

summary(LM_u_model)$r.square
```

> "This may seem small, but we must multiple it by $n$ to get the $LM$ statistic:"

$$LM = 2,725(0.0015)$$

```{r}
LM_test <- nobs(LM_u_model) * 0.0015
LM_test
```

> "The 10% critical value in a chi-square distribution with two degrees of freedom is about 4.61 (rounded to two decimal places)." 

```{r}
qchisq(1 - 0.10, 2)
```

> "Thus, we fail to reject the null hypothesis that $\beta_{avgsen} = 0$ and 
$\beta_{tottime} = 0$ at the 10% level." 

The _p_-value is:
$$P(X^2_{2} > 4.09) \approx 0.129$$
so we would reject the $H_0$ at the 15% level.

```{r}
1-pchisq(LM_test, 2)
```


## Chapter 6: Multiple Regression: Further Issues

**Example 6.1:` Effects of Pollution on Housing Prices, standardized.**

From the text:

> "We use the data $hrprice2$ to illustrate the use of beta coefficients. 
Recall that the key independent variable is $nox$, a measure of nitrogen oxide
in the air over each community. One way to understand the size of the pollution effect-without getting into the science underling nitrogen oxide's effect on air quality-is to compute beta coefficients. 
The population equation is the level-level model:"

$$price = \beta_0 + \beta_1nox + \beta_2crime + \beta_3rooms + \beta_4dist + \beta_5stratio + \mu$$


$price$: median housing price.

$nox$: Nitrous Oxide concentration; parts per million.

$crime$: number of reported crimes per capita.

$rooms$: average number of rooms in houses in the community.

$dist$: weighted distance of the community to 5 employment centers.

$stratio$: average student-teacher ratio of schools in the community.

 The beta coefficients are reported in the following equation (so each variable has been converted to its $z$-score):"

$$\widehat{zprice} = \beta_1znox + \beta_2zcrime + \beta_3zrooms + \beta_4zdist + \beta_5zstratio$$

First, load the `hrpice2` data.

```r{}
data(hrpice2)

```

Next, estimate the coefficient with the usual `lm` regression model but this time, standardized coefficients by wrapping each variable with R's `scale` function:

```{r, tidy = TRUE}
housing_standard <- lm(scale(price)~0+scale(nox)+scale(crime)+scale(rooms)+scale(dist) + scale(stratio), data = hprice2)

housing_standard$coefficients
```

**`Example 6.2:` Effects of Pollution on Housing Prices, Quadratic Interactive Term**

We modify the housing model, adding a quadratic term in _rooms_: 

$$log(price) = \beta_0 + \beta_1log(nox) + \beta_2log(dist) + \beta_3rooms + \beta_4rooms^2 + \beta_5stratio + \mu$$
```{r}
housing_interactive <- lm(lprice ~ lnox + log(dist) + rooms+I(rooms^2) + stratio, data = hprice2)

summary(housing_interactive)
```

## Chapter 7: Multiple Regression Analysis with Qualitative Information 

**`Example 7.4:` Housing Price Regression, Qualitative Binary variable**

This time we use the `hrpice1` data.

```r{}
data(hrpice1)
```

Having just worked with `hrpice2`, it may be helpful to view the documentation on this data set and read the variable names.

```{r, eval=FALSE}
?hprice1
```

$$\widehat{log(price)} = \beta_0 + \beta_1log(lotsize) + \beta_2log(sqrft) + \beta_3bdrms + \beta_4colonial $$

Estimate the coefficients of the above linear model on the `hprice` data set.

```{r, tidy=TRUE}
housing_qualitative <- lm(lprice ~ llotsize + lsqrft + bdrms + colonial, data = hprice1)

summary(housing_qualitative)
```

Summary from the text:

> "All the variables are self-explanatory except $colonial$, which is a binary variable equal to one if the house is of the colonial style. What does the coefficient on $colonial$ mean? For given levels of $lotsize$, $sqrt$, and $bdrms$, the difference in $\widehat{log(price)}$ between a house of colonial style and that of another style is 0.54. This means that colonial-style house is predicted to sell for about 5.4% more, holding other factors fixed."

## Chapter 8: Heteroskedasticity

**`Example 8.9:` Determinants of Personal Computer Ownership**

> "We use the data in $GPA1$ to estimate the probability of owning a computer.
Let $PC$ denote a binary indicator equal to unity if the student owns a computer, and zero otherwise. The variable $hsGPA$ is high school GPA, $ACT$ is achievement test score, and $parcoll$ is a binary indicator equal to unity if at least one parent attended college."

> "The equation estimated by OLS is:"

$$\widehat{PC} = \beta_0 + \beta_1hsGPA + \beta_2ACT + \beta_3parcoll + \beta_4colonial $$



Load the `gpa1` data and create a new variable combining the`fathcoll` and `mothcoll`, into one, `parcoll`. This new column indicates if any parent went to college, not just one or the other.

```{r}
data(GPA1)
gpa1$parcoll <- as.integer(gpa1$fathcoll==1 | gpa1$mothcoll)
```

```{r}
GPA_OLS <- lm(PC ~ hsGPA + ACT + parcoll, data = gpa1)

summary(GPA_OLS)
```

> "Just as with example 8.8, there are no striking differences between the usual and robust standard errors. Nevertheless, we also estimate the model by Weighted Least Squares or $WLS$. Because all of the $OLS$ fitted values are inside the unit interval, no adjustments are needed"

First, calculate the weights and then pass them to the same linear model.

```{r}
weights <- GPA_OLS$fitted.values * (1-GPA_OLS$fitted.values)

GPA_WLS <- lm(PC ~ hsGPA + ACT + parcoll, data = gpa1, weights = 1/weights)

summary(GPA_WLS)
```

> "There are no important differences in the OLS and WLS estimates. The only significant explanatory variable is $parcoll$, and in both cases we estimate that the probability of $PC$ ownership is about .22 higher if at least on parent attended college"


# Chapter 9: More on Specification and Data Issues

**`Example 9.8:` R&D Intensity and Firm Size**

> "Suppose the R&D expenditures as a percentage of sales, $rdintens$, are realted to $sales$ (in millions) and profits as a percentage of sales, $profmarg$:"

$$rdintens = \beta_0 + \beta_1sales + \beta_2profmarg + \mu$$

> "The $OLS$ equation using data on 32 chemical companies in $rdchem$ is"

Load the data, run the model, and apply the `summary` diagnostics function to the model.

```{r}
data(rdchem)
 
all_rdchem <- lm(rdintens ~ sales + profmarg, data = rdchem)

summary(all_rdchem)
```

Neither $sales$ nor $profmarg$ is statistically significant at even the 10% level in this regression. 

Of the 32 firms, 31 have annual sales less than $20$ billion. One firm has annual sales of almost $40$ billions. Figure 9.1 shows how far this firm is from the rest of the sample.


```{r, tidy=TRUE}
plot(rdintens ~ sales, pch = 21, bg = "lightblue", data = rdchem, 
     main = "FIGURE 9.1: Scatterplot of R&D intensity against firm sales", 
     xlab = "firm sales (in millions of dollars)", ylab = "R&D as a percentage of sales")
```

> "In terms of sales, this firm is over twice as large as every other firm, so it might be a good idea to estimate the model without it. When we do this, we obtain:"

```{r}
smallest_rdchem <- lm(rdintens ~ sales + profmarg, data = rdchem, 
                      subset = (sales < max(sales)))

summary(smallest_rdchem)
```

# Chapter 10: Basic Regression Analysis with Time Series Data

**`Example 10.2:` Effects of Inflation and Deficits on Interest Rates**

> "The data in INTDEF.RAW come from the 2004 Economic Report of the President (Tables B-73 and B-79) and span the years 1948 through 2003. The variable $i3$ is the three-month T-bill rate, $inf$ is the annual inflation rate based on the consumer price index (CPI), and $def$ is the federal budget deficit as a percentage of GDP. The estimated equation is:"

$$\widehat{i3} = \beta_0 + \beta_1inf_t + \beta_2def_t$$

```{r}
data("intdef")

tbill_model <- lm(i3 ~ inf + def, data = intdef)

summary(tbill_model)
```

> "These estimates show that increases in inflation or the relative size of the deficit increase short-term interest rates, both of which are expected from basic economics. For example, a ceteris paribus one percentage point increase in the inflation rate increases i3 by .606 points. Both inf and def are very statistically significant, assuming, of course, that the CLM assumptions hold."

**`Example 10.11:` Seasonal Effects of Antidumping Filings**

In _Example 10.5_, we used monthly data (in the file `BARIUM`) that have not been seasonally adjusted. 

```{r}
# Example 10.5

data("barium")

barium_model <- lm(lchnimp ~ lchempi + lgas + lrtwex + befile6 + affile6 + afdec6, data = barium)

summary(barium_model)
```

> "Therefore, we should add seasonal dummy variables to make sure none of the important conclusions change. It could be that the months just before the suit was filed are months where imports are higher or lower, on average, than in other months." 

```{r}

barium_seasonal <- lm(lchnimp ~ lchempi + lgas + lrtwex + befile6 + affile6 + afdec6 + feb + mar + apr + may + jun + jul + aug + sep + oct + nov + dec, data = barium)

barium_seasonal_hat <- lm(lchnimp ~ lchempi + lgas + lrtwex + befile6 + affile6 + afdec6, data = barium)

anova(barium_seasonal,barium_seasonal_hat)

```


> "When we add the 11 monthly dummy variables as in 10.41 and test their joint significance, we obtain $p-value 5 .5852$, and so the seasonal dummies are jointly insignificant. In addition, nothing important changes in the estimates once statistical significance is taken into account. Krupp and Pollard (1996) actually used three dummy variables for the seasons (fall, spring, and summer, with winter as the base season), rather than a full set of monthly dummies; the outcome is essentially the same."

