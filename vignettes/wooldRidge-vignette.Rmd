---
title: "wooldRidge-vignette"
author: "Justin M Shea"
date: "`r Sys.Date()`"
output: html_vignette
pdf_document: default
vignette: >
  %\VignetteIndexEntry{wooldRidge-vignette}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

An excellent approach to learning is to find an example from your textbook
and then recreate it. Below are examples from every chapter and the syntax 
provided here should get you through most of the book.

Load the `wooldRidge` package to access data in the manner specified in each example.
```{r, echo = TRUE, eval = TRUE, warning=FALSE}
library(wooldRidge)
```

## Chapter 2: The Simple Regression Model

**`Example 2.10:` A Log Wage Equation**

From the text:

> " Using the wage1 data as in Example 2.4, but using log(wage) as the dependent variable, we obtain the following relationship:"

$$\widehat{log(wage)} = \beta_0 + \beta_1educ$$

First, load the `wage1` data.
```{r}
data(wage1)
```

Next, estimate a linear relationship between the log of _wage_ and _education_.

```{r}
log_wage_model <- lm(lwage ~ educ, data = wage1)
```

Finally, print the coefficients and $R^2$.

```{r}
log_wage_model$coefficients
summary(log_wage_model)$r.squared
```

## Chapter 3: Multiple Regression Analysis: Estimation

**`Example 3.2:` Hourly Wage Equation**

From the text:

> " Using the 526 observations on workers in 'wage1', we include $educ$(years of education), $exper$(years of labor market experience), and $tenure$(years with the current employer) in an equation explain log($wage$)."

$$\widehat{log(wage)} = \beta_0 + \beta_1educ + \beta_3exper + \beta_4tenure$$

Estimate the model regressing _education_, _experience_, and _tenure_ against _log(wage)_.
```{r}
hourly_wage_model <- lm(lwage ~ educ + exper + tenure, data = wage1)
```

Again, print the estimated model coefficients:
```{r}
hourly_wage_model$coefficients
```


## Chapter 4: Multiple Regression Analysis: Inference

**`Example 4.7` Effect of Job Training on Firm Scrap Rates**

From the text:

> " The scrap rate for a manufacturing firm is the number of defective items - products that must be discarded - out of every 100 produced. Thus, for a given number of items produced, a decrease in the scrap rate reflects higher worker productivity."

> "We can use the scrap rate to measure the effect of worker training on productivity. Using the data in jtrain, but only for the year 1987 and for non-unionized firms, we obtain the following estimated equation:"

First, load the `jtrain` data set.
```{r, echo = TRUE, eval = TRUE, warning=FALSE}
data("jtrain")
```

Next, create a logical index identifying which observations occur in 1987 and are non-union.

```{r} 
index <- jtrain$year == 1987 & jtrain$union == 0
```

Next, subset the jtrain data by the new index. This returns a data.frame of `jtrain` data of non-union firms for the year 1987.

```{r}
jtrain_1987_nonunion <- jtrain[index,]
```

Now create the linear model regressing hrsemp(total hours training/total employees trained), the log of annual sales, and the log of the number of the employees, against the log of the scrape rate.

$$lscrap = \alpha + \beta_1 hrsemp + \beta_2 lsales + \beta_3 lemploy$$


```{r}
linear_model <- lm(lscrap ~ hrsemp + lsales + lemploy, data = jtrain_1987_nonunion)
```

Finally, print the complete summary statistic diagnostics of the model.
```{r}
summary(linear_model)
```

## Chapter 5: Multiple Regression Analysis: OLS Asymptotics

**`Example 5.3:` Economic Model of Crime**

From the text:

> "We illustrate the Lagrange Multiplier $(LM)$ statistics test by using a slight extension of the crime model from example 3.5."

$$narr86 = \beta_0 + \beta_1pcnv + \beta_2avgsen + \beta_3tottime + \beta_4ptime86 + \beta_5qemp86 + \mu$$

$narr86:$ number of times arrested, 1986.

$pcnv:$ proportion of prior arrests leading to convictions.

$avgsen:$ average sentence served, length in months.

$tottime:$ time in prison since reaching the age of 18, length in months.

$ptime86:$ months in prison during 1986

$qemp86:$ quarters employed, 1986


Load the `crime1` data set containing arrests during the year 1986 and other information on 2,725 men born in either 1960 or 1961 in California.
```{r}
data(crime1)
```

From the text:

> "We use the $LM$ statistic to test the null hypothesis that $avgsen$ and $tottime$ have no effect on $narr86$ once other factors have been controlled for. First, estimate the restricted model by regressing $narr86$ on $pcnv, ptime86,$ and $qemp86$; the variables $avgsen$ and $tottime$ are excluded from this regression."

```{r, tidy = TRUE}
restricted_model <- lm(narr86 ~ pcnv + ptime86 + qemp86, data = crime1)
```

We obtain the residuals $\tilde{\mu}$ from this regression, 2,725 of them. 

```{r}
restricted_model_u <- restricted_model$residuals
```

Next, we run the regression of:

$$\tilde{\mu} = \beta_1pcnv + \beta_2avgsen + \beta_3tottime + \beta_4ptime86 + \beta_5qemp86$$
From the text:

> "As always, the order in which we list the independent variables is irrelevant.This second regression produces $R^2_{\mu}$, which turns out to be about 0.0015."

```{r, tidy = TRUE}
LM_u_model <- lm(restricted_model_u ~ pcnv + ptime86 + qemp86 + avgsen + tottime, data = crime1)

summary(LM_u_model)$r.square
```

> "This may seem small, but we must multiple it by $n$ to get the $LM$ statistic:"

$$LM = 2,725(0.0015)$$

```{r}
LM_test <- nobs(LM_u_model) * 0.0015
LM_test
```

> "The 10% critical value in a chi-square distribution with two degrees of freedom is about 4.61 (rounded to two decimal places)." 

```{r}
qchisq(1 - 0.10, 2)
```

> "Thus, we fail to reject the null hypothesis that $\beta_{avgsen} = 0$ and 
$\beta_{tottime} = 0$ at the 10% level." 

The _p_-value is:
$$P(X^2_{2} > 4.09) \approx 0.129$$
so we would reject the $H_0$ at the 15% level.

```{r}
1-pchisq(LM_test, 2)
```


## Chapter 6: Multiple Regression: Further Issues

**Example 6.1:` Effects of Pollution on Housing Prices, standardized.**

From the text:

> "We use the data $hrprice2$ to illustrate the use of beta coefficients. 
Recall that the key independent variable is $nox$, a measure of nitrogen oxide
in the air over each community. One way to understand the size of the pollution effect-without getting into the science underling nitrogen oxide's effect on air quality-is to compute beta coefficients. 
The population equation is the level-level model:"

$$price = \beta_0 + \beta_1nox + \beta_2crime + \beta_3rooms + \beta_4dist + \beta_5stratio + \mu$$


$price$: median housing price.

$nox$: Nitrous Oxide concentration; parts per million.

$crime$: number of reported crimes per capita.

$rooms$: average number of rooms in houses in the community.

$dist$: weighted distance of the community to 5 employment centers.

$stratio$: average student-teacher ratio of schools in the community.

 The beta coefficients are reported in the following equation (so each variable has been converted to its $z$-score):"

$$\widehat{zprice} = \beta_1znox + \beta_2zcrime + \beta_3zrooms + \beta_4zdist + \beta_5zstratio$$

First, load the `hrpice2` data.

```r{}
data(hrpice2)

```

Next, estimate the coefficient with the usual `lm` regression model but this time, standardized coefficients by wrapping each variable with R's `scale` function:

```{r, tidy = TRUE}
housing_standard <- lm(scale(price)~0+scale(nox)+scale(crime)+scale(rooms)+scale(dist) + scale(stratio), data = hprice2)

housing_standard$coefficients
```

**`Example 6.2:` Effects of Pollution on Housing Prices, Quadratic Interactive Term**

We modify the housing model, adding a quadratic term in _rooms_: 

$$log(price) = \beta_0 + \beta_1log(nox) + \beta_2log(dist) + \beta_3rooms + \beta_4rooms^2 + \beta_5stratio + \mu$$
```{r}
housing_interactive <- lm(lprice ~ lnox + log(dist) + rooms+I(rooms^2) + stratio, data = hprice2)

summary(housing_interactive)
```

## Chapter 7: Multiple Regression Analysis with Qualitative Information 

**`Example 7.4:` Housing Price Regression, Qualitative Binary variable**

This time we use the `hrpice1` data.

```r{}
data(hrpice1)
```

Having just worked with `hrpice2`, it may be helpful to view the documentation on this data set and read the variable names.

```{r, eval=FALSE}
?hprice1
```

$$\widehat{log(price)} = \beta_0 + \beta_1log(lotsize) + \beta_2log(sqrft) + \beta_3bdrms + \beta_4colonial $$

Estimate the coefficients of the above linear model on the `hprice` data set.

```{r, tidy=TRUE}
housing_qualitative <- lm(lprice ~ llotsize + lsqrft + bdrms + colonial, data = hprice1)

summary(housing_qualitative)
```

Summary from the text:

> "All the variables are self-explanatory except $colonial$, which is a binary variable equal to one if the house is of the colonial style. What does the coefficient on $colonial$ mean? For given levels of $lotsize$, $sqrt$, and $bdrms$, the difference in $\widehat{log(price)}$ between a house of colonial style and that of another style is 0.54. This means that colonial-style house is predicted to sell for about 5.4% more, holding other factors fixed."

## Chapter 8: Heteroskedasticity

**`Example 8.9:` Determinants of Personal Computer Ownership**

> "We use the data in $GPA1$ to estimate the probability of owning a computer.
Let $PC$ denote a binary indicator equal to unity if the student owns a computer, and zero otherwise. The variable $hsGPA$ is high school GPA, $ACT$ is achievement test score, and $parcoll$ is a binary indicator equal to unity if at least one parent attended college."

> "The equation estimated by OLS is:"

$$\widehat{PC} = \beta_0 + \beta_1hsGPA + \beta_2ACT + \beta_3parcoll + \beta_4colonial $$



Load the `gpa1` data and create a new variable combining the`fathcoll` and `mothcoll`, into one, `parcoll`. This new column indicates if any parent went to college, not just one or the other.

```{r}
data(GPA1)
gpa1$parcoll <- as.integer(gpa1$fathcoll==1 | gpa1$mothcoll)
```

```{r}
GPA_OLS <- lm(PC ~ hsGPA + ACT + parcoll, data = gpa1)

summary(GPA_OLS)
```

> "Just as with example 8.8, there are no striking differences between the usual and robust standard errors. Nevertheless, we also estimate the model by Weighted Least Squares or $WLS$. Because all of the $OLS$ fitted values are inside the unit interval, no adjustments are needed"

First, calculate the weights and then pass them to the same linear model.

```{r}
weights <- GPA_OLS$fitted.values * (1-GPA_OLS$fitted.values)

GPA_WLS <- lm(PC ~ hsGPA + ACT + parcoll, data = gpa1, weights = 1/weights)

summary(GPA_WLS)
```

> "There are no important differences in the OLS and WLS estimates. The only significant explanatory variable is $parcoll$, and in both cases we estimate that the probability of $PC$ ownership is about .22 higher if at least on parent attended college"


## Chapter 9: More on Specification and Data Issues

**`Example 9.8:` R&D Intensity and Firm Size**

> "Suppose the R&D expenditures as a percentage of sales, $rdintens$, are realted to $sales$ (in millions) and profits as a percentage of sales, $profmarg$:"

$$rdintens = \beta_0 + \beta_1sales + \beta_2profmarg + \mu$$

> "The $OLS$ equation using data on 32 chemical companies in $rdchem$ is"

Load the data, run the model, and apply the `summary` diagnostics function to the model.

```{r}
data(rdchem)
 
all_rdchem <- lm(rdintens ~ sales + profmarg, data = rdchem)

summary(all_rdchem)
```

Neither $sales$ nor $profmarg$ is statistically significant at even the 10% level in this regression. 

Of the 32 firms, 31 have annual sales less than $20$ billion. One firm has annual sales of almost $40$ billions. Figure 9.1 shows how far this firm is from the rest of the sample.


```{r, tidy=TRUE}
plot(rdintens ~ sales, pch = 21, bg = "lightblue", data = rdchem, 
     main = "FIGURE 9.1: Scatterplot of R&D intensity against firm sales", 
     xlab = "firm sales (in millions of dollars)", ylab = "R&D as a percentage of sales")
```

> "In terms of sales, this firm is over twice as large as every other firm, so it might be a good idea to estimate the model without it. When we do this, we obtain:"

```{r}
smallest_rdchem <- lm(rdintens ~ sales + profmarg, data = rdchem, 
                      subset = (sales < max(sales)))

summary(smallest_rdchem)
```

## Chapter 10: Basic Regression Analysis with Time Series Data

**`Example 10.2:` Effects of Inflation and Deficits on Interest Rates**

> "The data in INTDEF.RAW come from the 2004 Economic Report of the President (Tables B-73 and B-79) and span the years 1948 through 2003. The variable $i3$ is the three-month T-bill rate, $inf$ is the annual inflation rate based on the consumer price index (CPI), and $def$ is the federal budget deficit as a percentage of GDP. The estimated equation is:"

$$\widehat{i3} = \beta_0 + \beta_1inf_t + \beta_2def_t$$

```{r}
data("intdef")

tbill_model <- lm(i3 ~ inf + def, data = intdef)

summary(tbill_model)
```

> "These estimates show that increases in inflation or the relative size of the deficit increase short-term interest rates, both of which are expected from basic economics. For example, a ceteris paribus one percentage point increase in the inflation rate increases i3 by .606 points. Both inf and def are very statistically significant, assuming, of course, that the CLM assumptions hold."

**`Example 10.11:` Seasonal Effects of Antidumping Filings**

In _Example 10.5_, we used monthly data (in the file `BARIUM`) that have not been seasonally adjusted. 

```{r}
# Example 10.5
data("barium")

lm(lchnimp ~ lchempi + lgas + lrtwex + befile6 + affile6 + afdec6, data = barium)
```

> "Therefore, we should add seasonal dummy variables to make sure none of the important conclusions change. It could be that the months just before the suit was filed are months where imports are higher or lower, on average, than in other months." 

```{r}
barium_seasonal <- lm(lchnimp ~ lchempi + lgas + lrtwex + befile6 + affile6 + afdec6 + feb + mar + apr + may + jun + jul + aug + sep + oct + nov + dec, data = barium)

barium_seasonal_hat <- lm(lchnimp ~ lchempi + lgas + lrtwex + befile6 + affile6 + afdec6, data = barium)

anova(barium_seasonal, barium_seasonal_hat)

```


> "When we add the 11 monthly dummy variables as in 10.41 and test their joint significance, we obtain $p-value = 5 .5852$, and so the seasonal dummies are jointly insignificant. In addition, nothing important changes in the estimates once statistical significance is taken into account. Krupp and Pollard (1996) actually used three dummy variables for the seasons (fall, spring, and summer, with winter as the base season), rather than a full set of monthly dummies; the outcome is essentially the same."

## Chapter 11: Further Issues in Using OLS with with Time Series Data

**`Example 11.7:` Wages and Productivity**

> "The variable $hrwage$ is average hourly wage in the U.S. economy, and $outphr$ is output per
hour. One way to estimate the elasticity of hourly wage with respect to output per hour is
to estimate the equation:"


$$\widehat{log(hrwage_t)} = \beta_0 + \beta_1log(outphr_t) + \beta_2t + \mu_t$$

> "where the time trend is included because $log(hrwage)$ and $log(outphr)$ both display clear,
upward, linear trends. Using the data in 'EARNS' for the years 1947 through 1987, we obtain:"

```{r}
data("earns")

wage_time <- lm(lhrwage ~ loutphr + t, data = earns)
        
summary(wage_time)
````

> "(We have reported the usual goodness-of-fit measures here; it would be better to report those based on the detrended dependent variable, as in Section 10.5.). The estimated elasticity seems too large: a 1% increase in productivity increases real wages by about 1.64%. Because the standard error is so small, the 95% confidence interval easily excludes a unit elasticity. U.S. workers would probably have trouble believing that their wages increase by more than 1.5% for every 1% increase in productivity."

> "The regression results must be viewed with caution. Even after linearly detrending $log(hrwage)$, the first order autocorrelation is .967, and for detrended $log(outphr), \hat{p} = 0.945$. These suggest that both series have unit roots, so we reestimate the equation in first differences (and we no longer need a time trend):"

```{r}
wage_diff <- lm(diff(lhrwage) ~ diff(loutphr), data = earns)

summary(wage_diff)
````

> "Now, a 1% increase in productivity is estimated to increase real wages by about 0.81%, and the estimate is not statistically different from one. The adjusted R‑squared shows that the growth in output explains about 35% of the growth in real wages."

## Chapter 12: Serial Correlation and Heteroskedasticiy in Time Series Regressions

**`Example 12.4:` Prais-Winsten Estimation in the Event Study**

> "Again using the data in BARIUM, we estimate the equation in Example 10.5 using iterated Prais-Winsten estimation."

> "The coefficients that are statistically significant in the Prais-Winsten estimation do
not differ by much from the OLS estimates [in particular, the coefficients on $log(chempi)$, $log(rtwex)$, and $afdec6$]. It is not surprising for statistically insignificant coefficients to change, perhaps markedly, across different estimation methods.

First, run the linear model from example 10.5 and 10.11. 

```{r}
data("barium")
# Example 10.5
barium_linear_model <- lm(lchnimp ~ lchempi + lgas + lrtwex + befile6 + affile6 + afdec6, data = barium)

barium_linear_model
```

Then load the `prais` package and use the `prais.winsten` function to estimate the same model. Print the names of both models to the console to compare the results of both.

```{r}
library(prais)
barium_prais_winsten <- prais.winsten(lchnimp ~ lchempi + lgas + lrtwex + befile6 + affile6 + afdec6, data = barium)

barium_prais_winsten
```


> "Notice how the standard errors in the second column are uniformly higher than the
standard errors in column (1). This is common. The Prais-Winsten standard errors account
for serial correlation; the $OLS$ standard errors do not. As we saw in Section 12.1, the OLS standard errors usually understate the actual sampling variation in the OLS estimates and should not be relied upon when significant serial correlation is present. Therefore, the effect on Chinese imports after the International Trade Commission’s decision is now less statistically significant than we thought."

> "Finally, an R-squared is reported he $PW$ estimation that is well below the
R-squared for the $OLS$ estimation in this case. However, these R-squareds should not be
compared. For $OLS$, the R-squared, as usual, is based on the regression with the untransformed dependent and independent variables. For $PW$, the R-squared comes from the final regression of the $transformed$ dendent variable on the transformed independent vari-ables. It is not clear what this $R^2$ actually measuring; nevertheless, it is traditionally reported."

**`Example 12.8:` Heteroskedasticity and the Efficient Markets Hypothesis**

> "In Example 11.4, we estimated the simple $AR(1)$ model:"

$$return_t = \beta_0 + \beta_1return_{t-1} + \mu_t$$

> "The EMH states that $\beta_1 = 0$. When we tested this hypothesis using the data in 'NYSE', we obtained $t_b{1} = 1.55$ with $n = 689$. 

```{r}
data("nyse")
 
return_AR <-lm(return ~ return_1, data = nyse)

summary(return_AR)
```


> "With such a large sample, this is not much evidence against the EMH. Although the EMH states that the expected return given past observable information should be constant, it says nothing about the conditional variance. In fact, the Breusch-Pagan test for heteroskedasticity entails regressing the squared $OLS$ residuals $\hat{\mu^2_t}$ on $return_{t-1}$""


$$\hat{\mu^2_t} = \beta_0 + \beta_1return_{t-1} + residual_t$$

Calculated $\hat{\mu^2_t}$ by taking the residuals contained in the `return_AR` model object and store the results in the variable named `return_mu`. Then regress the `return_1` variable against the square of `return_mu`. Notice, we set data equal to the `return_AR` objects model matrix, which contains data free of leading missing values inherent to lagged variables.

```{r}
return_mu <- residuals(return_AR)

mu2_hat_model <- lm(return_mu^2 ~ return_1, data = return_AR$model)

summary(mu2_hat_model)
```

> "The $t$ statistic on $return_{t-1}$ is about -5.5, indicating strong evidence of heteroskedasticity. Because the coeffict on $return_{t-1}$ is negative, we have the interesting finding that volatility in stock returns is lower the previous return was high, and vice versa. Therefore, we have found what is common in many financial studies: the expected value of stock returns does not depend on past returns, but the variance of returns does."

**`Example 12.9:` ARCH in Stock Returns**

> "In Example 12.8, we saw that there was heteroskedasticity in weekly stock returns. This
heteroskedasticity is actually better characterized by the ARCH model in (12.50). If we
compute the OLS residuals from (12.47), square these, and regress them on the lagged
squared residual, we obtain:"

$$\hat{\mu^2_t} = \beta_0 + \hat{\mu^2_{t-1}} + residual_t$$

We still have `return_mu` in the working environment so we can use it to create $\hat{\mu^2_t}$, (`mu2_hat`) and $\hat{\mu^2_{t-1}}$ (`mu2_hat_1`). Notice the use `R`'s matrix subset operations to perform the lag operation. We drop the first observation of `mu2_hat` and squared the results. Next, we remove the last observation of `mu2_hat_1` using the subtraction operator combined with a call to the `NROW` function on `return_mu`. Now, both contain $688$ observations and we can run a standard linear model.

```{r}
mu2_hat  <- return_mu[-1]^2

mu2_hat_1 <- return_mu[-NROW(return_mu)]^2

arch_model <- lm(mu2_hat ~ mu2_hat_1)

summary(arch_model)
```

> "The t statistic on $\hat{\mu^2_{t-1}}$ (mu2_hat_1) is over nine, indicating strong ARCH. As we discussed earlier, a larger error at time $t-1$ implies a larger variance in stock returns today.

> "It is important to see that, though the $squared$ $OLS$ residuals are autocorrelated, the $OLS$ residuals themselves are not (as is consistent with the EMH). Regressing on $\hat{\mu_t}$ and $\hat{\mu_{t-1}}$ gives $\hat{p} = 0.0014$ with $t_{\hat{p}} = 0.038$.

## Chapter 13: Pooling Cross Sections across Time: Simple Panel Data Methods

**`Example 13.7:` Effect of Drunk Driving Laws on Traffic Fatalities**

> "Many states in the United States have adopted different policies in an attempt to curb
drunk driving. Two types of laws that we will study here are $open$ $container$ $laws$ -which make it illegal for passengers to have open containers of alcoholic beverages -and $administrative$ $per$ $se$ $laws$ -which allow courts to suspend licenses after a driver is arrested for drunk driving but before the driver is convicted. One possible analysis is to use a single cross section of states to regress driving fatalities (or those related to drunk driving) on dummy variable indicators for whether each law is present. This is unlikely to work well because states decide, through legislative processes, whether they need such laws. Therefore, the presence of laws is likely to be related to the average drunk driving fatalities in recent years. A more convincing analysis uses panel data over a time period where some states adopted new laws (and some states may have repealed existing laws). The
file TRAFFIC1 contains data for 1985 and 1990 for all 50 states and the District of
Columbia. The dependent variable is the number of traffic deaths per 100 million miles
driven (dthrte). In 1985, 19 states had open container laws while 22 states had such laws
in 1990. In 1985, 21 states had per se laws; the number had grown to 29 by 1990.
Using OLS after first differencing gives:"

$$\widehat{\Delta{dthrte}} = \beta_0 + \Delta{open} + \Delta{admin}$$

```{r}
data("traffic1")
 
DD_model <- lm(cdthrte ~ copen + cadmn, data = traffic1)

summary(DD_model)
```

> "The estimates suggest that adopting an open container law lowered the traffic fatality rate by $0.42$, a nontrivial effect given that the average death rate in 1985 was
2.7 with a standard deviation of about 0.6. The estimate is statistically significant at the 5% level against a twosided alternative. The administrative per se law has a smaller effect, and its t statistic is only -1.29; but the estimate is the sign we expect. The intercept in this equation shows that traffic fatalities fell substantially for all states over the five-year period, whether or not there were any law changes. The states that adopted an open container law over this period saw a further drop, on average, in fatality rates."

> "Other laws might also affect traffic fatalities, such as seat belt laws, motorcycle helmet laws, and maximum speed limits. In addition, we might want to control for age and gender distributions, as well as measures of how influential an organization such as Mothers Against Drunk Driving is in each state."

## Chapter 14: Advanced Panel Data Methods

**`Example 14.1:` Effect of Job Training on Firm Scrap Rates**

> "We use the data for three years, 1987, 1988, and 1989, on the 54 firms that reported scrap rates in each year. No firms received grants prior to 1988; in 1988, 19 firms received grants; in 1989, 10 different firms received grants. Therefore, we must also allow for the possibility that the additional job training in 1988 made workers more productive in 1989. This is easily done by including a lagged value of the grant indicator. We also include year dummies for 1988 and 1989. The results are given in Table below. 

Install the `plm` package and check out the documentation. The model syntax for `plm` models is very similar to the linear model, with additional slots to further define various estimation methods.
```{r, tidy=TRUE}
library(plm)

data("jtrain")
 
scrap_panel <- plm(lscrap ~ d88 + d89 + grant + grant_1, data = jtrain,
            index = c('fcode','year'), model = 'within', effect ='individual')

summary(scrap_panel)
```


> "We have reported the results in a way that emphasizes the need to interpret the estimates in light of the unobserved effects model, (14.4). We are explicitly controlling for the unobserved, time-constant effects in $\alpha_i$. The time-demeaning allows us to estimate the $\Beta_j$, but (14.5) is not the best equation for iterpreting the estimates.

> "Interestingly, the estimated lagged effect of the training grant is substantially
larger than the contemporaneous effect: job training has an effect at least one year
later. Because the dependent variable is in logarithmic form, obtaining a grant in
1988 is predicted to lower the firm scrap rate in 1989 by about 34.4% $[exp(-0.422)-1 = -0.344]$; the coefficient on $grant_1$ is significant at the 5% level against a twosided alternative. The coefficient $grant$ is significant at the 10% level, and the
size of the coefficient is hardly trivial. Notice the $df$ is obtained as $N(T-1) - k = 54(3-1)-4 = 104$"

> "The coefficient on $d89$ indicates that the scrap rate was substantially lower in 1989
than in the base year, 1987, even in the absence of job training grants. Thus, it is important to allow for these aggregate effects. If we omitted the year dummies, the secular increase in worker productivity would be attributed to the job training grants.
The diagnostic results above shows that, even after controlling for aggregate trends in productivity, the job training grants had a large estimated effect."

> "Finally, it is crucial to allow for the lagged effect in the model. If we omit $grant_1$, then we are assuming that the effect of job training does not last into the next year. The estimate on $grant$ when we drop $grant_1$ is -0.082 ($t = -0.65$); this is much smaller and statistically insignificant."




